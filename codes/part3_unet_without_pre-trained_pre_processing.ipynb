{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fe6a03",
   "metadata": {},
   "source": [
    "# UNET training without transfer learning pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff058fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "required = {'opencv-python'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing], stdout=None)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "from patchify import patchify, unpatchify\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# Albumentations\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from model import UNET\n",
    "\n",
    "from data_loader import Dataloader_trdp\n",
    "from utils import (\n",
    "    load_checkpoint,\n",
    "    load_checkpoint_pspnetlite,\n",
    "    save_checkpoint,\n",
    "    #get_loaders,\n",
    "    check_accuracy,\n",
    "    save_predictions_as_imgs,\n",
    "    predict,\n",
    "    store_predictions,\n",
    "    store_predictions_with_patching,\n",
    "    store_predictions_unet_improved\n",
    ")\n",
    "\n",
    "# Clean graphics memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e986c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 1\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "chip_dimension = 256\n",
    "LOAD_MODEL = True\n",
    "TRAIN = False\n",
    "TEST = False\n",
    "SAVE = False\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "PREDICT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25d7b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 1504 - Test: 624\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "train_dir  = Path('../../DATASET/dataset_pruebas/train')\n",
    "test_dir  = Path('../../DATASET/dataset_pruebas/validation')\n",
    "\n",
    "\n",
    "train_dir = Path('../../DATASET/archive/train_final')\n",
    "test_dir  = Path('../../DATASET/archive/val_final')\n",
    "\n",
    "\n",
    "csv_file = Path('./output_csvs/df_train_untidy.csv')\n",
    "csv_file_test = Path('./output_csvs/df_test_untidy.csv')\n",
    "\n",
    "\n",
    "csv_file = Path('./registros/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "csv_file_test = Path('./registros/output_csvs_dataset_prueba/df_test_untidy.csv')\n",
    "'''\n",
    "\n",
    "#train_dir = Path('../../DATASET/archive/train_final')\n",
    "\n",
    "train_dir  = Path('../../DATASET/dataset_pruebas/train')\n",
    "test_dir  = Path('../../DATASET/dataset_pruebas/validation')\n",
    "#test_dir  = Path('../../DATASET/archive/val_final')\n",
    "\n",
    "#csv_file = Path('./output_csvs/df_train_untidy.csv')\n",
    "csv_file = Path('./registros/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "csv_file_test = Path('./registros/output_csvs_dataset_prueba/df_test_untidy.csv')\n",
    "\n",
    "#csv_file_test = Path('./output_csvs_all/df_test_untidy.csv')\n",
    "\n",
    "root_dir  = train_dir\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df_test = pd.read_csv(csv_file_test)\n",
    "\n",
    "\n",
    "sample_dir = Path('../../DATASET/SN7_buildings_train_sample')\n",
    "\n",
    "# Reconstructiontest_loader\n",
    "img_size = 1024\n",
    "# Chip size given batch_size\n",
    "chip_dim = ((img_size -1)//batch_size + 1)*2\n",
    "# number of Columns per chip\n",
    "columns = img_size / chip_dim\n",
    "# Needed patches to reconstruct original image\n",
    "patches_total = int(columns**2)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "A.Normalize(mean=mean,std=std)\n",
    "#A.Rotate(limit=(-360, 360), interpolation=4, border_mode=4,p=1),\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        #A.Resize(height=256, width=256),\n",
    "        \n",
    "        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n",
    "        #A.RandomRotate90(p=1.0),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set   = Dataloader_trdp(root_dir=train_dir,csv_file=csv_file,chip_dimension=chip_dimension,transform=transform)\n",
    "\n",
    "testing_set = Dataloader_trdp(root_dir=test_dir,csv_file=csv_file_test,chip_dimension=chip_dimension,transform=transform)\n",
    "\n",
    "#train_set, val_set = torch.utils.data.random_split(train_set, [round(0.7*len(train_set)),round(0.3*len(train_set))])\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle = False)\n",
    "#val_loader   = DataLoader(dataset = val_set, batch_size=batch_size, shuffle = False)\n",
    "test_loader  = DataLoader(dataset = testing_set, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "print(f\"Train : {len(train_loader)} - Test: {len(test_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5da0ef",
   "metadata": {},
   "source": [
    "\n",
    "# UNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3599020f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.10.0\n",
      "Total params: 124M\n",
      "=> Loading checkpoint\n",
      "Model loaded successfully\n",
      " 0/624 \n",
      " 1/624 \n",
      " 2/624 \n",
      " 3/624 \n",
      " 4/624 \n",
      " 5/624 \n",
      " 6/624 \n",
      " 7/624 \n",
      " 8/624 \n",
      " 9/624 \n",
      " 10/624 \n",
      " 11/624 \n",
      " 12/624 \n",
      " 13/624 \n",
      " 14/624 \n",
      " 15/624 \n",
      " 16/624 \n",
      " 17/624 \n",
      " 18/624 \n",
      " 19/624 \n",
      " 20/624 \n",
      " 21/624 \n",
      " 22/624 \n",
      " 23/624 \n",
      " 24/624 \n",
      " 25/624 \n",
      " 26/624 \n",
      " 27/624 \n",
      " 28/624 \n",
      " 29/624 \n",
      " 30/624 \n",
      " 31/624 \n",
      " 32/624 \n",
      " 33/624 \n",
      " 34/624 \n",
      " 35/624 \n",
      " 36/624 \n",
      " 37/624 \n",
      " 38/624 \n",
      " 39/624 \n",
      " 40/624 \n",
      " 41/624 \n",
      " 42/624 \n",
      " 43/624 \n",
      " 44/624 \n",
      " 45/624 \n",
      " 46/624 \n",
      " 47/624 \n",
      " 48/624 \n",
      " 49/624 \n",
      " 50/624 \n",
      " 51/624 \n",
      " 52/624 \n",
      " 53/624 \n",
      " 54/624 \n",
      " 55/624 \n",
      " 56/624 \n",
      " 57/624 \n",
      " 58/624 \n",
      " 59/624 \n",
      " 60/624 \n",
      " 61/624 \n",
      " 62/624 \n",
      " 63/624 \n",
      " 64/624 \n",
      " 65/624 \n",
      " 66/624 \n",
      " 67/624 \n",
      " 68/624 \n",
      " 69/624 \n",
      " 70/624 \n",
      " 71/624 \n",
      " 72/624 \n",
      " 73/624 \n",
      " 74/624 \n",
      " 75/624 \n",
      " 76/624 \n",
      " 77/624 \n",
      " 78/624 \n",
      " 79/624 \n",
      " 80/624 \n",
      " 81/624 \n",
      " 82/624 \n",
      " 83/624 \n",
      " 84/624 \n",
      " 85/624 \n",
      " 86/624 \n",
      " 87/624 \n",
      " 88/624 \n",
      " 89/624 \n",
      " 90/624 \n",
      " 91/624 \n",
      " 92/624 \n",
      " 93/624 \n",
      " 94/624 \n",
      " 95/624 \n",
      " 96/624 \n",
      " 97/624 \n",
      " 98/624 \n",
      " 99/624 \n",
      " 100/624 \n",
      " 101/624 \n",
      " 102/624 \n",
      " 103/624 \n",
      " 104/624 \n",
      " 105/624 \n",
      " 106/624 \n",
      " 107/624 \n",
      " 108/624 \n",
      " 109/624 \n",
      " 110/624 \n",
      " 111/624 \n",
      " 112/624 \n",
      " 113/624 \n",
      " 114/624 \n",
      " 115/624 \n",
      " 116/624 \n",
      " 117/624 \n",
      " 118/624 \n",
      " 119/624 \n",
      " 120/624 \n",
      " 121/624 \n",
      " 122/624 \n",
      " 123/624 \n",
      " 124/624 \n",
      " 125/624 \n",
      " 126/624 \n",
      " 127/624 \n",
      " 128/624 \n",
      " 129/624 \n",
      " 130/624 \n",
      " 131/624 \n",
      " 132/624 \n",
      " 133/624 \n",
      " 134/624 \n",
      " 135/624 \n",
      " 136/624 \n",
      " 137/624 \n",
      " 138/624 \n",
      " 139/624 \n",
      " 140/624 \n",
      " 141/624 \n",
      " 142/624 \n",
      " 143/624 \n",
      " 144/624 \n",
      " 145/624 \n",
      " 146/624 \n",
      " 147/624 \n",
      " 148/624 \n",
      " 149/624 \n",
      " 150/624 \n",
      " 151/624 \n",
      " 152/624 \n",
      " 153/624 \n",
      " 154/624 \n",
      " 155/624 \n",
      " 156/624 \n",
      " 157/624 \n",
      " 158/624 \n",
      " 159/624 \n",
      " 160/624 \n",
      " 161/624 \n",
      " 162/624 \n",
      " 163/624 \n",
      " 164/624 \n",
      " 165/624 \n",
      " 166/624 \n",
      " 167/624 \n",
      " 168/624 \n",
      " 169/624 \n",
      " 170/624 \n",
      " 171/624 \n",
      " 172/624 \n",
      " 173/624 \n",
      " 174/624 \n",
      " 175/624 \n",
      " 176/624 \n",
      " 177/624 \n",
      " 178/624 \n",
      " 179/624 \n",
      " 180/624 \n",
      " 181/624 \n",
      " 182/624 \n",
      " 183/624 \n",
      " 184/624 \n",
      " 185/624 \n",
      " 186/624 \n",
      " 187/624 \n",
      " 188/624 \n",
      " 189/624 \n",
      " 190/624 \n",
      " 191/624 \n",
      " 192/624 \n",
      " 193/624 \n",
      " 194/624 \n",
      " 195/624 \n",
      " 196/624 \n",
      " 197/624 \n",
      " 198/624 \n",
      " 199/624 \n",
      " 200/624 \n",
      " 201/624 \n",
      " 202/624 \n",
      " 203/624 \n",
      " 204/624 \n",
      " 205/624 \n",
      " 206/624 \n",
      " 207/624 \n",
      " 208/624 \n",
      " 209/624 \n",
      " 210/624 \n",
      " 211/624 \n",
      " 212/624 \n",
      " 213/624 \n",
      " 214/624 \n",
      " 215/624 \n",
      " 216/624 \n",
      " 217/624 \n",
      " 218/624 \n",
      " 219/624 \n",
      " 220/624 \n",
      " 221/624 \n",
      " 222/624 \n",
      " 223/624 \n",
      " 224/624 \n",
      " 225/624 \n",
      " 226/624 \n",
      " 227/624 \n",
      " 228/624 \n",
      " 229/624 \n",
      " 230/624 \n",
      " 231/624 \n",
      " 232/624 \n",
      " 233/624 \n",
      " 234/624 \n",
      " 235/624 \n",
      " 236/624 \n",
      " 237/624 \n",
      " 238/624 \n",
      " 239/624 \n",
      " 240/624 \n",
      " 241/624 \n",
      " 242/624 \n",
      " 243/624 \n",
      " 244/624 \n",
      " 245/624 \n",
      " 246/624 \n",
      " 247/624 \n",
      " 248/624 \n",
      " 249/624 \n",
      " 250/624 \n",
      " 251/624 \n",
      " 252/624 \n",
      " 253/624 \n",
      " 254/624 \n",
      " 255/624 \n",
      " 256/624 \n",
      " 257/624 \n",
      " 258/624 \n",
      " 259/624 \n",
      " 260/624 \n",
      " 261/624 \n",
      " 262/624 \n",
      " 263/624 \n",
      " 264/624 \n",
      " 265/624 \n",
      " 266/624 \n",
      " 267/624 \n",
      " 268/624 \n",
      " 269/624 \n",
      " 270/624 \n",
      " 271/624 \n",
      " 272/624 \n",
      " 273/624 \n",
      " 274/624 \n",
      " 275/624 \n",
      " 276/624 \n",
      " 277/624 \n",
      " 278/624 \n",
      " 279/624 \n",
      " 280/624 \n",
      " 281/624 \n",
      " 282/624 \n",
      " 283/624 \n",
      " 284/624 \n",
      " 285/624 \n",
      " 286/624 \n",
      " 287/624 \n",
      " 288/624 \n",
      " 289/624 \n",
      " 290/624 \n",
      " 291/624 \n",
      " 292/624 \n",
      " 293/624 \n",
      " 294/624 \n",
      " 295/624 \n",
      " 296/624 \n",
      " 297/624 \n",
      " 298/624 \n",
      " 299/624 \n",
      " 300/624 \n",
      " 301/624 \n",
      " 302/624 \n",
      " 303/624 \n",
      " 304/624 \n",
      " 305/624 \n",
      " 306/624 \n",
      " 307/624 \n",
      " 308/624 \n",
      " 309/624 \n",
      " 310/624 \n",
      " 311/624 \n",
      " 312/624 \n",
      " 313/624 \n",
      " 314/624 \n",
      " 315/624 \n",
      " 316/624 \n",
      " 317/624 \n",
      " 318/624 \n",
      " 319/624 \n",
      " 320/624 \n",
      " 321/624 \n",
      " 322/624 \n",
      " 323/624 \n",
      " 324/624 \n",
      " 325/624 \n",
      " 326/624 \n",
      " 327/624 \n",
      " 328/624 \n",
      " 329/624 \n",
      " 330/624 \n",
      " 331/624 \n",
      " 332/624 \n",
      " 333/624 \n",
      " 334/624 \n",
      " 335/624 \n",
      " 336/624 \n",
      " 337/624 \n",
      " 338/624 \n",
      " 339/624 \n",
      " 340/624 \n",
      " 341/624 \n",
      " 342/624 \n",
      " 343/624 \n",
      " 344/624 \n",
      " 345/624 \n",
      " 346/624 \n",
      " 347/624 \n",
      " 348/624 \n",
      " 349/624 \n",
      " 350/624 \n",
      " 351/624 \n",
      " 352/624 \n",
      " 353/624 \n",
      " 354/624 \n",
      " 355/624 \n",
      " 356/624 \n",
      " 357/624 \n",
      " 358/624 \n",
      " 359/624 \n",
      " 360/624 \n",
      " 361/624 \n",
      " 362/624 \n",
      " 363/624 \n",
      " 364/624 \n",
      " 365/624 \n",
      " 366/624 \n",
      " 367/624 \n",
      " 368/624 \n",
      " 369/624 \n",
      " 370/624 \n",
      " 371/624 \n",
      " 372/624 \n",
      " 373/624 \n",
      " 374/624 \n",
      " 375/624 \n",
      " 376/624 \n",
      " 377/624 \n",
      " 378/624 \n",
      " 379/624 \n",
      " 380/624 \n",
      " 381/624 \n",
      " 382/624 \n",
      " 383/624 \n",
      " 384/624 \n",
      " 385/624 \n",
      " 386/624 \n",
      " 387/624 \n",
      " 388/624 \n",
      " 389/624 \n",
      " 390/624 \n",
      " 391/624 \n",
      " 392/624 \n",
      " 393/624 \n",
      " 394/624 \n",
      " 395/624 \n",
      " 396/624 \n",
      " 397/624 \n",
      " 398/624 \n",
      " 399/624 \n",
      " 400/624 \n",
      " 401/624 \n",
      " 402/624 \n",
      " 403/624 \n",
      " 404/624 \n",
      " 405/624 \n",
      " 406/624 \n",
      " 407/624 \n",
      " 408/624 \n",
      " 409/624 \n",
      " 410/624 \n",
      " 411/624 \n",
      " 412/624 \n",
      " 413/624 \n",
      " 414/624 \n",
      " 415/624 \n",
      " 416/624 \n",
      " 417/624 \n",
      " 418/624 \n",
      " 419/624 \n",
      " 420/624 \n",
      " 421/624 \n",
      " 422/624 \n",
      " 423/624 \n",
      " 424/624 \n",
      " 425/624 \n",
      " 426/624 \n",
      " 427/624 \n",
      " 428/624 \n",
      " 429/624 \n",
      " 430/624 \n",
      " 431/624 \n",
      " 432/624 \n",
      " 433/624 \n",
      " 434/624 \n",
      " 435/624 \n",
      " 436/624 \n",
      " 437/624 \n",
      " 438/624 \n",
      " 439/624 \n",
      " 440/624 \n",
      " 441/624 \n",
      " 442/624 \n",
      " 443/624 \n",
      " 444/624 \n",
      " 445/624 \n",
      " 446/624 \n",
      " 447/624 \n",
      " 448/624 \n",
      " 449/624 \n",
      " 450/624 \n",
      " 451/624 \n",
      " 452/624 \n",
      " 453/624 \n",
      " 454/624 \n",
      " 455/624 \n",
      " 456/624 \n",
      " 457/624 \n",
      " 458/624 \n",
      " 459/624 \n",
      " 460/624 \n",
      " 461/624 \n",
      " 462/624 \n",
      " 463/624 \n",
      " 464/624 \n",
      " 465/624 \n",
      " 466/624 \n",
      " 467/624 \n",
      " 468/624 \n",
      " 469/624 \n",
      " 470/624 \n",
      " 471/624 \n",
      " 472/624 \n",
      " 473/624 \n",
      " 474/624 \n",
      " 475/624 \n",
      " 476/624 \n",
      " 477/624 \n",
      " 478/624 \n",
      " 479/624 \n",
      " 480/624 \n",
      " 481/624 \n",
      " 482/624 \n",
      " 483/624 \n",
      " 484/624 \n",
      " 485/624 \n",
      " 486/624 \n",
      " 487/624 \n",
      " 488/624 \n",
      " 489/624 \n",
      " 490/624 \n",
      " 491/624 \n",
      " 492/624 \n",
      " 493/624 \n",
      " 494/624 \n",
      " 495/624 \n",
      " 496/624 \n",
      " 497/624 \n",
      " 498/624 \n",
      " 499/624 \n",
      " 500/624 \n",
      " 501/624 \n",
      " 502/624 \n",
      " 503/624 \n",
      " 504/624 \n",
      " 505/624 \n",
      " 506/624 \n",
      " 507/624 \n",
      " 508/624 \n",
      " 509/624 \n",
      " 510/624 \n",
      " 511/624 \n",
      " 512/624 \n",
      " 513/624 \n",
      " 514/624 \n",
      " 515/624 \n",
      " 516/624 \n",
      " 517/624 \n",
      " 518/624 \n",
      " 519/624 \n",
      " 520/624 \n",
      " 521/624 \n",
      " 522/624 \n",
      " 523/624 \n",
      " 524/624 \n",
      " 525/624 \n",
      " 526/624 \n",
      " 527/624 \n",
      " 528/624 \n",
      " 529/624 \n",
      " 530/624 \n",
      " 531/624 \n",
      " 532/624 \n",
      " 533/624 \n",
      " 534/624 \n",
      " 535/624 \n",
      " 536/624 \n",
      " 537/624 \n",
      " 538/624 \n",
      " 539/624 \n",
      " 540/624 \n",
      " 541/624 \n",
      " 542/624 \n",
      " 543/624 \n",
      " 544/624 \n",
      " 545/624 \n",
      " 546/624 \n",
      " 547/624 \n",
      " 548/624 \n",
      " 549/624 \n",
      " 550/624 \n",
      " 551/624 \n",
      " 552/624 \n",
      " 553/624 \n",
      " 554/624 \n",
      " 555/624 \n",
      " 556/624 \n",
      " 557/624 \n",
      " 558/624 \n",
      " 559/624 \n",
      " 560/624 \n",
      " 561/624 \n",
      " 562/624 \n",
      " 563/624 \n",
      " 564/624 \n",
      " 565/624 \n",
      " 566/624 \n",
      " 567/624 \n",
      " 568/624 \n",
      " 569/624 \n",
      " 570/624 \n",
      " 571/624 \n",
      " 572/624 \n",
      " 573/624 \n",
      " 574/624 \n",
      " 575/624 \n",
      " 576/624 \n",
      " 577/624 \n",
      " 578/624 \n",
      " 579/624 \n",
      " 580/624 \n",
      " 581/624 \n",
      " 582/624 \n",
      " 583/624 \n",
      " 584/624 \n",
      " 585/624 \n",
      " 586/624 \n",
      " 587/624 \n",
      " 588/624 \n",
      " 589/624 \n",
      " 590/624 \n",
      " 591/624 \n",
      " 592/624 \n",
      " 593/624 \n",
      " 594/624 \n",
      " 595/624 \n",
      " 596/624 \n",
      " 597/624 \n",
      " 598/624 \n",
      " 599/624 \n",
      " 600/624 \n",
      " 601/624 \n",
      " 602/624 \n",
      " 603/624 \n",
      " 604/624 \n",
      " 605/624 \n",
      " 606/624 \n",
      " 607/624 \n",
      " 608/624 \n",
      " 609/624 \n",
      " 610/624 \n",
      " 611/624 \n",
      " 612/624 \n",
      " 613/624 \n",
      " 614/624 \n",
      " 615/624 \n",
      " 616/624 \n",
      " 617/624 \n",
      " 618/624 \n",
      " 619/624 \n",
      " 620/624 \n",
      " 621/624 \n",
      " 622/624 \n",
      " 623/624 \n",
      "\n",
      "Test set (624): Average loss: -13.4064, mIoU: 0.3129\n",
      "\n",
      "y_pred sizes: (624, 256, 256) - true_pred sizes: (624, 256, 256)\n",
      "y_pred pixel values: [  0 255]\n",
      "Data size will be reconstructed successfully.\n",
      "Prediction (0)\n",
      "Prediction (1)\n",
      "Prediction (2)\n",
      "Prediction (3)\n",
      "Prediction (4)\n",
      "Prediction (5)\n",
      "Prediction (6)\n",
      "Prediction (7)\n",
      "Prediction (8)\n",
      "Prediction (9)\n",
      "Prediction (10)\n",
      "Prediction (11)\n",
      "Prediction (12)\n",
      "Prediction (13)\n",
      "Prediction (14)\n",
      "Prediction (15)\n",
      "Prediction (16)\n",
      "Prediction (17)\n",
      "Prediction (18)\n",
      "Prediction (19)\n",
      "Prediction (20)\n",
      "Prediction (21)\n",
      "Prediction (22)\n",
      "Prediction (23)\n",
      "Prediction (24)\n",
      "Prediction (25)\n",
      "Prediction (26)\n",
      "Prediction (27)\n",
      "Prediction (28)\n",
      "Prediction (29)\n",
      "Prediction (30)\n",
      "Prediction (31)\n",
      "Prediction (32)\n",
      "Prediction (33)\n",
      "Prediction (34)\n",
      "Prediction (35)\n",
      "Prediction (36)\n",
      "Prediction (37)\n",
      "Prediction (38)\n",
      "True label (0)\n",
      "True label (1)\n",
      "True label (2)\n",
      "True label (3)\n",
      "True label (4)\n",
      "True label (5)\n",
      "True label (6)\n",
      "True label (7)\n",
      "True label (8)\n",
      "True label (9)\n",
      "True label (10)\n",
      "True label (11)\n",
      "True label (12)\n",
      "True label (13)\n",
      "True label (14)\n",
      "True label (15)\n",
      "True label (16)\n",
      "True label (17)\n",
      "True label (18)\n",
      "True label (19)\n",
      "True label (20)\n",
      "True label (21)\n",
      "True label (22)\n",
      "True label (23)\n",
      "True label (24)\n",
      "True label (25)\n",
      "True label (26)\n",
      "True label (27)\n",
      "True label (28)\n",
      "True label (29)\n",
      "True label (30)\n",
      "True label (31)\n",
      "True label (32)\n",
      "True label (33)\n",
      "True label (34)\n",
      "True label (35)\n",
      "True label (36)\n",
      "True label (37)\n",
      "True label (38)\n",
      "Ended segmentations..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nfor epoch in range(1, args.epoch + 1):\\n    st = time.time()\\n    scheduler.step()    \\n    # train for one epoch\\n    print(\"Unet improved ==> testing, epoch \" + str(epoch))\\n    # test\\n    loss_per_epoch_test, acc_val_per_epoch_i = testing(args, model, device, test_loader)\\n\\n    loss_test_epoch += loss_per_epoch_test\\n    acc_test_per_epoch += [acc_val_per_epoch_i]\\n\\n\\n    if epoch == 1:\\n        best_acc_val = acc_val_per_epoch_i\\n\\n    else:\\n        if acc_val_per_epoch_i > best_acc_val:\\n            best_acc_val = acc_val_per_epoch_i\\n\\n\\n    np.save(res_path + \\'/\\' + \\'LOSS_epoch_val.npy\\', np.asarray(loss_test_epoch))\\n\\n    # save accuracies:\\n    np.save(res_path + \\'/\\' + \\'accuracy_per_epoch_val.npy\\', np.asarray(acc_test_per_epoch))\\n    \\n    cont += 1\\n\\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\\n\\n# To delete temporal files\\nimport shutil\\n\\nshutil.rmtree(\"outputs\")\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import numpy as np\n",
    "print('PyTorch version:', torch.__version__)\n",
    "\n",
    "class configuration:\n",
    "    def __init__(self):\n",
    "        self.experiment_name = \"trdp1.001\"\n",
    "        self.pre_load = \"True\" ## Load dataset in memory\n",
    "        self.pre_trained = \"True\"\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_label = 255\n",
    "        self.lr = learning_rate  # 0.001 if pretrained. 0.1 if scratch\n",
    "        self.M = [] ##If training from scratch, reduce learning rate at some point\n",
    "        self.batch_size = batch_size  # Training batch size\n",
    "        self.test_batch_size = 4  # Test batch size\n",
    "        self.epoch = num_epochs ## Number of epochs\n",
    "        self.train_root = \"./VOC\"\n",
    "        self.download = False\n",
    "        self.seed = 271828\n",
    "\n",
    "\n",
    "## Create arguments object\n",
    "args = configuration()\n",
    "\n",
    "# Make sure to enable GPU acceleration!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Set random seed for reproducability\n",
    "torch.backends.cudnn.deterministic = True  # fix the GPU to deterministic mode\n",
    "torch.manual_seed(args.seed)  # CPU seed\n",
    "torch.cuda.manual_seed_all(args.seed)  # GPU seed\n",
    "random.seed(args.seed)  # python seed for image transformation\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "def _fast_hist(label_pred, label_true, num_classes):\n",
    "    mask = (label_true >= 0) & (label_true < num_classes)\n",
    "    hist = np.bincount(\n",
    "        num_classes * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n",
    "    return hist\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"Fast enough iou calculation function\"\"\"\n",
    "    SMOOTH = 1e-6\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    #outputs = outputs.detach()\n",
    "    #labels = labels.detach()\n",
    "    \n",
    "    intersection = (outputs & labels).float().sum((1, 2))\n",
    "    union = (outputs | labels).float().sum((1, 2))\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded.mean() # to get a batch averageimport pdb\n",
    "\n",
    "\n",
    "def train_SemanticSeg(args, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    counter = 1\n",
    "\n",
    "    gts_all, predictions_all = [], []\n",
    "\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        \n",
    "        images = data[\"raster_diff\"].float()\n",
    "        \n",
    "        mask = data[\"mask_diff\"].float()\n",
    "        \n",
    "        images, mask = images.to(device), mask.to(device).squeeze()\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images).squeeze()\n",
    "\n",
    "        \n",
    "        #Aggregated per-pixel loss\n",
    "        loss = criterion(outputs, mask)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        iou = iou_pytorch(mask.unsqueeze(0).int(), outputs.int())\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Learning rate: {:.6f}'.format(\n",
    "                epoch, int(counter * len(images)), len(train_loader.dataset),\n",
    "                100. * counter / len(train_loader), loss.item(),\n",
    "                optimizer.param_groups[0]['lr']))\n",
    "            print(f\"Iou: {iou}\")\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return sum(train_loss) / len(train_loss)#, mean_iu\n",
    "        \n",
    "        \n",
    "def testing(args, model, device, test_loader, criterion):\n",
    "\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    loss_per_batch = []\n",
    "    test_loss = 0\n",
    "    \n",
    "    gts_all, predictions_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data) in enumerate(test_loader):\n",
    "            \n",
    "            print(f\" {batch_idx}/{len(test_loader)} \")\n",
    "            if batch_idx <= 5001:\n",
    "            \n",
    "                images = data[\"raster_diff\"].float()\n",
    "                mask = data[\"mask_diff\"].float()#type(torch.LongTensor)\n",
    "                images, mask = images.to(device), mask.to(device).squeeze()\n",
    "                #Forward pass\n",
    "                outputs = model(images).squeeze()\n",
    "                #outputs = outputs.clone().detach().cpu().numpy()\n",
    "        #        outputs = outputs.cpu().numpy()\n",
    "                outputs = ((outputs - outputs.min())/(outputs.max()-outputs.min()))\n",
    "                #outputs = np.round(outputs)\n",
    "\n",
    "                outputs[outputs>THRESHOLD] = 1 \n",
    "                outputs[outputs<=THRESHOLD] = 0\n",
    "                #outputs = torch.round(outputs)\n",
    "\n",
    "                #Aggregated per-pixel loss\n",
    "                loss = criterion(outputs, mask)\n",
    "                loss_per_batch.append(loss.item())\n",
    "\n",
    "                # Convert to numpy\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                outputs = outputs.astype(\"int64\")\n",
    "                #predictions = np.round(predictions)\n",
    "\n",
    "                mask = mask.data.cpu().numpy()\n",
    "                mask = mask.astype(\"int64\")\n",
    "\n",
    "                '''\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.imshow(outputs)\n",
    "                np.unique(outputs)\n",
    "\n",
    "                import seaborn as sns\n",
    "                sns.histplot(data = outputs)\n",
    "\n",
    "                preds = outputs.data.max(1)[1].squeeze(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                mask = mask.data.cpu().numpy()\n",
    "                mask = mask.astype(\"int64\")\n",
    "                plt.imshow(mask)\n",
    "\n",
    "                assert np.unique(mask) == np.unique(outputs*255)\n",
    "                '''\n",
    "                gts_all.append(mask)\n",
    "                predictions_all.append(255*outputs)\n",
    "\n",
    "                if SAVE:\n",
    "\n",
    "                    images = \"outputs\"\n",
    "                    labels = images+\"/predictions\"\n",
    "                    true = images+\"/labels\"\n",
    "\n",
    "                    if not os.path.exists(images):\n",
    "                        os.makedirs(images)\n",
    "                    if not os.path.exists(labels):\n",
    "                            os.makedirs(labels)\n",
    "                    if not os.path.exists(true):\n",
    "                            os.makedirs(true)\n",
    "\n",
    "                    matplotlib.image.imsave(f\"{labels}/Pred_{batch_idx}.png\", outputs , cmap='gray')\n",
    "\n",
    "                    matplotlib.image.imsave(f\"{true}/True_{batch_idx}.png\", mask, cmap='gray')\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "    loss_per_epoch = [np.average(loss_per_batch)]\n",
    "    \n",
    "\n",
    "    ##Compute Mean Intersection over Union (mIoU)\n",
    "    ##mIoU: Mean (of all classes) of intersection over union between prediction\n",
    "    ##and ground-truth\n",
    "\n",
    "    # Improved: https://towardsdatascience.com/intersection-over-union-iou-calculation-for-evaluating-an-image-segmentation-model-8b22e2e84686 \n",
    "    iou_scores = []\n",
    "    for lp, lt in zip(predictions_all, gts_all):\n",
    "        # Convert lp to cpu\n",
    "        #lp = lp.detach().cpu().numpy()\n",
    "        intersection = np.logical_and(lp.flatten(), lt.flatten())  \n",
    "        union = np.logical_or(lp.flatten(), lt.flatten())  \n",
    "        iou_score = np.sum(intersection) / np.sum(union)\n",
    "        iou_scores.append(iou_score)\n",
    "    \n",
    "    mean_iu = np.average(iou_scores)\n",
    "    \n",
    "    print('\\nTest set ({:.0f}): Average loss: {:.4f}, mIoU: {:.4f}\\n'.format(\n",
    "        len(test_loader.dataset), loss_per_epoch[-1], mean_iu)) \n",
    "    \n",
    "    ###########################################################################\n",
    "    # Store predictions\n",
    "\n",
    "    return loss_per_epoch, mean_iu, predictions_all, gts_all\n",
    "    \n",
    "\n",
    "model = UNET(in_channels=3, out_channels=1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print('Total params: %2.fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "    \n",
    "milestones = args.M\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "\n",
    "import os\n",
    "import time\n",
    "loss_train_epoch = []\n",
    "loss_test_epoch = []\n",
    "acc_train_per_epoch = []\n",
    "acc_test_per_epoch = []\n",
    "new_labels = []\n",
    "\n",
    "cont = 0\n",
    "\n",
    "res_path = \"./metrics_\" + args.experiment_name\n",
    "\n",
    "\n",
    "filename = \"unet_17_dic_DATASET_PRUEBA_1_epochs_works.pth\" # my_checkpoint.pth.tar\n",
    "model_path = \"./models\" # + filename\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if LOAD_MODEL:\n",
    "        load_checkpoint_pspnetlite(torch.load(\"models/\" + filename), model)\n",
    "    \n",
    "\n",
    "if not os.path.isdir(res_path):\n",
    "    os.makedirs(res_path)\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "        st = time.time()\n",
    "        scheduler.step()    \n",
    "        # train for one epoch\n",
    "        if TRAIN:\n",
    "\n",
    "            print(\"Unet improved --> training, epoch \" + str(epoch))\n",
    "    \n",
    "            loss_per_epoch = train_SemanticSeg(args, model, device, train_loader, optimizer, epoch, criterion)\n",
    "    \n",
    "            loss_train_epoch += [loss_per_epoch]\n",
    "            \n",
    "            if epoch == args.epoch:\n",
    "                torch.save(model.state_dict(), \"models/\" + filename)\n",
    "    \n",
    "            np.save(res_path + '/' + 'LOSS_epoch_train.npy', np.asarray(loss_train_epoch))\n",
    "        \n",
    "        # TESTING\n",
    "        if TEST:\n",
    "            print(\"Unet improved ==> testing, epoch \" + str(epoch))\n",
    "            # test\n",
    "            loss_per_epoch_test, acc_val_per_epoch_i, a,b = testing(args, model, device, test_loader, criterion)\n",
    "    \n",
    "            loss_test_epoch += loss_per_epoch_test\n",
    "            acc_test_per_epoch += [acc_val_per_epoch_i]\n",
    "    \n",
    "    \n",
    "            if epoch == 1:\n",
    "                best_acc_val = acc_val_per_epoch_i\n",
    "    \n",
    "            else:\n",
    "                if acc_val_per_epoch_i > best_acc_val:\n",
    "                    best_acc_val = acc_val_per_epoch_i\n",
    "    \n",
    "    \n",
    "            np.save(res_path + '/' + 'LOSS_epoch_val.npy', np.asarray(loss_test_epoch))\n",
    "    \n",
    "            # save accuracies:\n",
    "            np.save(res_path + '/' + 'accuracy_per_epoch_val.npy', np.asarray(acc_test_per_epoch))\n",
    "            \n",
    "            cont += 1\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "    \n",
    "# PREDICT\n",
    "\n",
    "if PREDICT: \n",
    "    \n",
    "    y_pred, true_pred = [], []\n",
    "    \n",
    "    loss_per_epoch_test, acc_val_per_epoch_i, y_pred, true_pred = testing(args, model, device, test_loader, criterion)\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    true_pred = np.array(true_pred)\n",
    "    \n",
    "    print(f\"y_pred sizes: {y_pred.shape} - true_pred sizes: {true_pred.shape}\")\n",
    "    print(f\"y_pred pixel values: {np.unique(y_pred)}\")\n",
    "\n",
    "    store_predictions_with_patching(y_pred, true_pred, 16)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    st = time.time()\n",
    "    scheduler.step()    \n",
    "    # train for one epoch\n",
    "    print(\"Unet improved ==> testing, epoch \" + str(epoch))\n",
    "    # test\n",
    "    loss_per_epoch_test, acc_val_per_epoch_i = testing(args, model, device, test_loader)\n",
    "\n",
    "    loss_test_epoch += loss_per_epoch_test\n",
    "    acc_test_per_epoch += [acc_val_per_epoch_i]\n",
    "\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_acc_val = acc_val_per_epoch_i\n",
    "\n",
    "    else:\n",
    "        if acc_val_per_epoch_i > best_acc_val:\n",
    "            best_acc_val = acc_val_per_epoch_i\n",
    "\n",
    "\n",
    "    np.save(res_path + '/' + 'LOSS_epoch_val.npy', np.asarray(loss_test_epoch))\n",
    "\n",
    "    # save accuracies:\n",
    "    np.save(res_path + '/' + 'accuracy_per_epoch_val.npy', np.asarray(acc_test_per_epoch))\n",
    "    \n",
    "    cont += 1\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "\n",
    "# To delete temporal files\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"outputs\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94276746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505260f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
