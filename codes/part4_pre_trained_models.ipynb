{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff058fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "required = {'opencv-python'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing], stdout=None)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "from patchify import patchify, unpatchify\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import rasterio as rio\n",
    "from rasterio import features\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import geopandas as gpd\n",
    "from descartes import PolygonPatch\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import rasterio as rio\n",
    "# Albumentations\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from model import UNET, UNET_manual\n",
    "\n",
    "from data_loader import Dataloader_trdp\n",
    "from utils import (\n",
    "    load_checkpoint,\n",
    "    load_checkpoint_pspnetlite,\n",
    "    save_checkpoint,\n",
    "    #get_loaders,\n",
    "    check_accuracy,\n",
    "    save_predictions_as_imgs,\n",
    "    predict,\n",
    "    store_predictions,\n",
    "    store_predictions_with_patching,\n",
    "    store_predictions_unet_improved\n",
    ")\n",
    "\n",
    "# Clean graphics memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#import imgaug\n",
    "import random\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #imgaug.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "'''\n",
    "def force_cudnn_initialization():\n",
    "    s = 32\n",
    "    dev = torch.device('cuda')\n",
    "    torch.nn.functional.conv2d(torch.zeros(s, s, s, s, device=dev), torch.zeros(s, s, s, s, device=dev))\n",
    "\n",
    "force_cudnn_initialization()\n",
    "'''\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 1\n",
    "learning_rate = 0.005\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "chip_dimension = 256\n",
    "LOAD_MODEL = False\n",
    "TRAIN = True\n",
    "TEST = False\n",
    "SAVE = False\n",
    "PREDICT = True\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.3\n",
    "\n",
    "filename = \"unet_16_dic_DATASET_full_1_epochs_works_dice_mixed.pth\" # my_checkpoint.pth.tar\n",
    "\n",
    "'''\n",
    "train_dir  = Path('../../DATASET/dataset_pruebas/train')\n",
    "test_dir  = Path('../../DATASET/dataset_pruebas/validation')\n",
    "\n",
    "\n",
    "train_dir = Path('../../DATASET/archive/train_final')\n",
    "test_dir  = Path('../../DATASET/archive/val_final')\n",
    "\n",
    "\n",
    "csv_file = Path('./output_csvs/df_train_untidy.csv')\n",
    "csv_file_test = Path('./output_csvs/df_test_untidy.csv')\n",
    "\n",
    "\n",
    "csv_file = Path('./registros/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "csv_file_test = Path('./registros/output_csvs_dataset_prueba/df_test_untidy.csv')\n",
    "'''\n",
    "\n",
    "#train_dir = Path('../../DATASET/archive/train_final')\n",
    "\n",
    "train_dir  = Path('../../DATASET/dataset_pruebas/train')\n",
    "test_dir  = Path('../../DATASET/dataset_pruebas/validation')\n",
    "#test_dir  = Path('../../DATASET/archive/val_final')\n",
    "\n",
    "#csv_file = Path('./output_csvs/df_train_untidy.csv')\n",
    "csv_file = Path('./registros/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "csv_file_test = Path('./registros/output_csvs_dataset_prueba/df_test_untidy.csv')\n",
    "\n",
    "#csv_file_test = Path('./output_csvs/df_test_untidy.csv')\n",
    "\n",
    "root_dir  = train_dir\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df_test = pd.read_csv(csv_file_test)\n",
    "\n",
    "\n",
    "sample_dir = Path('../../DATASET/SN7_buildings_train_sample')\n",
    "\n",
    "# Reconstructiontest_loader\n",
    "img_size = 1024\n",
    "# Chip size given batch_size\n",
    "chip_dim = ((img_size -1)//batch_size + 1)*2\n",
    "# number of Columns per chip\n",
    "columns = img_size / chip_dim\n",
    "# Needed patches to reconstruct original image\n",
    "patches_total = int(columns**2)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "A.Normalize(mean=mean,std=std)\n",
    "#A.Rotate(limit=(-360, 360), interpolation=4, border_mode=4,p=1),\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        #A.Resize(height=256, width=256),\n",
    "        \n",
    "        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n",
    "        #A.RandomRotate90(p=1.0),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af878cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler # custom dataset handling\n",
    "import torch.autograd.profiler as profiler # to track model inference and detect leaks\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms as T\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "import torchvision.models as models\n",
    "from torch import optim\n",
    "from collections import OrderedDict\n",
    "import segmentation_models_pytorch as smp #semantic segmentation models and utils\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class Dataloader_trdp_pre_trained(Dataset):\n",
    "    \"\"\"SpaceNet 7 Multi-Temporal Satellite Imagery Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self,csv_file, root_dir, no_udm=True, transform=None, chip_dimension=None, \n",
    "            preprocessing=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (Path): Path to the csv file with annotations\n",
    "            root_dir (Path): Parent directory containing all other directories.\n",
    "            no_udm (bool): Specifies whether the dataset will load UDM images or not.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample. This will be used for data augmentation\n",
    "            chip_dimension (int, optional): Specifies the dimensions of the chip being generated.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.no_udm = no_udm\n",
    "        self.transform = transform\n",
    "        self.total_images_index = self.total_number_of_images()\n",
    "        self.chip_dimension = chip_dimension\n",
    "        self.preprocessing = preprocessing\n",
    "        if self.chip_dimension is not None:\n",
    "            self.chip_generator = self.__ChipGenerator(chip_dimension = self.chip_dimension)\n",
    "            # this will be replaced later with an abstracted version\n",
    "            # returns number of chips per image assuming all images are 1024\n",
    "            # We will obtain the total number of chips as a patching manual method\n",
    "            self.n_chips = ((1024 - 1) // self.chip_dimension + 1)**2\n",
    "    '''\n",
    "    __len__ gets the total number of patches as the number of images available multiply by the number of splits n_chips\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        if self.chip_dimension is not None: # Total # images = No clouds + Not none\n",
    "            return len(self.total_images_index)*self.n_chips\n",
    "        else:\n",
    "            # if no patching is perform, image remain constant\n",
    "            return len(self.total_images_index)\n",
    "    '''\n",
    "    __getitem__ receives the index idx for each element of the dataset, interatevely.\n",
    "    raster_idx is the index for patches: number of idx available over the number of chips\n",
    "    chips_idx is the module of index with number of chips\n",
    "    '''\n",
    "    def __getitem__(self,idx):\n",
    "        if self.chip_dimension is not None:\n",
    "            raster_idx = idx//self.n_chips\n",
    "            chip_idx = idx%self.n_chips\n",
    "        else:\n",
    "            raster_idx = idx\n",
    "            \n",
    "        if torch.is_tensor(raster_idx):\n",
    "            raster_idx = raster_idx.tolist()\n",
    "        # Obtain the corresponding index to the desired iteration of the raster\n",
    "        idx1 = self.total_images_index[raster_idx]\n",
    "        # paths where the images are stored: containing UDM mask\n",
    "        img1_path = self.root_dir/self.annotations.loc[idx1,'images_masked']\n",
    "        # paths where the corresponding true building footprints \n",
    "        labels1_path = self.root_dir/self.annotations.loc[idx1,'labels_match_pix']\n",
    "        # read rasters using imported rasterio library\n",
    "        with rio.open(img1_path) as r1:\n",
    "            raster1 = r1.read()[0:3]  \n",
    "        raster_diff = raster1 \n",
    "        # get the dates for the images\n",
    "        date1 = tuple(self.annotations.loc[idx1,['month','year']])\n",
    "        # read geojson files for each of the satellite images into a geodataframe\n",
    "        gdf1 = gpd.read_file(labels1_path).set_index('Id').sort_index()\n",
    "        gdf_diff = gdf1  # self.__geo_difference(labels1_path,labels2_path)\n",
    "        # get the corresponding rasterized image of the geodataframes\n",
    "        mask_diff = self.__rasterize_gdf(gdf_diff,out_shape=raster1.shape[1:3])\n",
    "        \n",
    "        if self.chip_dimension:\n",
    "            raster_diff_dict = self.chip_generator(raster_diff)\n",
    "            mask_diff_dict = self.chip_generator(mask_diff)\n",
    "\n",
    "            sample = {'raster_diff':raster_diff_dict['chip'][chip_idx],'date1':date1,\n",
    "          'mask_diff':mask_diff_dict['chip'][chip_idx],'im_dir':str(img1_path.parent.parent),'blank_label':mask_diff_dict['blank'][chip_idx]}\n",
    "        \n",
    "        else:\n",
    "            sample = {'raster_diff':raster_diff,'date1':date1, 'mask_diff':mask_diff,'im_dir':str(img1_path.parent.parent)}\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            # get the individual images and mask from the output sample\n",
    "            raster1 = np.moveaxis(np.uint8(sample['raster_diff'][:3]),0,-1)\n",
    "            mask = np.moveaxis(np.uint8(sample['mask_diff']),0,-1)\n",
    "            import random\n",
    "            seed = random.randint(0,1000)\n",
    "            set_seed(seed)\n",
    "            \n",
    "            # apply transform on first image and mask\n",
    "            transformed = self.transform(image=raster1,mask=mask)\n",
    "            raster1 = transformed['image']\n",
    "            mask_diff = transformed['mask']\n",
    "            \n",
    "            set_seed(seed)\n",
    "            \n",
    "            # concatenate input images\n",
    "            raster_diff = raster1\n",
    "            # update sample dictionary paramters after transformation\n",
    "            if not isinstance(raster_diff,np.ndarray):\n",
    "                sample['raster_diff'] = raster_diff\n",
    "                mask_diff = mask_diff.permute(2,0,1)\n",
    "                sample['mask_diff'] = mask_diff\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                sample['raster_diff'] = raster_diff\n",
    "                mask_diff = np.moveaxis(mask_diff,-1,0)\n",
    "                sample['mask_diff'] = mask_diff\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing is not None:\n",
    "            raster1 = np.moveaxis(np.uint8(sample['raster_diff'][:3]),0,-1)\n",
    "            mask = np.moveaxis(np.uint8(sample['mask_diff']),0,-1)\n",
    "            sample = self.preprocessing(image=raster1, mask=mask)\n",
    "            image= sample['image']\n",
    "            mask = sample['mask']\n",
    "            sample['raster_diff'] = image\n",
    "            sample['mask_diff'] = mask\n",
    "            \n",
    "        return image,mask\n",
    "    \n",
    "    '''\n",
    "    total_number_of_images obtains the indexes for the images_masks, this can be changed by updating the groupby. \n",
    "    If UDM filter is available to discard cloudy images.\n",
    "    OUTPUT: indexes for selected folder. \n",
    "    '''\n",
    "    def total_number_of_images(self):\n",
    "            # we need to change it to obtain all the possible images we are going to use\n",
    "            total_images = []\n",
    "            # group by satellite image location\n",
    "            location_groups = self.annotations.groupby('images')\n",
    "            for i,location in enumerate(location_groups):\n",
    "                # get the dataframe in the group\n",
    "                loc_frame = location[1]\n",
    "                # make sure that list does not contain images with unidentified masks\n",
    "                condition = (loc_frame['has_udm'] == False)\n",
    "                # return a list of the indices in the location dataframe\n",
    "                l = list(loc_frame[condition].index)\n",
    "                total_images.extend(l)\n",
    "            return total_images \n",
    "    \n",
    "\n",
    "    \n",
    "    def __rasterize_gdf(self,gdf,out_shape):\n",
    "        # if geodataframe is empty return empty mask\n",
    "        if len(gdf)==0:\n",
    "            return np.zeros((1,*out_shape))\n",
    "            \n",
    "        mask = features.rasterize(((polygon, 255) for polygon in gdf['geometry']),out_shape=out_shape)\n",
    "        \n",
    "        return np.expand_dims(mask,axis=0)\n",
    "    \n",
    "    class __ChipGenerator():   \n",
    "        def __init__(self, chip_dimension=256,return_raster=False):  \n",
    "            self.chip_dimension = chip_dimension\n",
    "            self.return_raster = return_raster\n",
    "            self.chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n",
    "\n",
    "        def __call__(self,raster):\n",
    "            np_array = self.__read_raster(raster)\n",
    "            # get number of chips per colomn\n",
    "            n_rows = (np_array.shape[1] - 1) // self.chip_dimension + 1\n",
    "            # get number of chips per row\n",
    "            n_cols = (np_array.shape[2] - 1) // self.chip_dimension + 1\n",
    "            # segment image into chips and return dict of chips and metadata\n",
    "            chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n",
    "\n",
    "            for r in range(n_rows):\n",
    "                for c in range(n_cols):\n",
    "                    start_r_idx = r*self.chip_dimension\n",
    "                    end_r_idx = start_r_idx + self.chip_dimension\n",
    "\n",
    "                    start_c_idx = c*self.chip_dimension\n",
    "                    end_c_idx = start_c_idx + self.chip_dimension\n",
    "                    \n",
    "                    chip = np_array[:,start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n",
    "\n",
    "                    chip_dict['chip'].append(chip)\n",
    "                    chip_dict['x'].append(start_r_idx)\n",
    "                    chip_dict['y'].append(start_c_idx)\n",
    "                    \n",
    "                    # Check if the chip is an empty chip\n",
    "                    if chip.mean() == 0 and chip.sum() == 0:\n",
    "                        chip_dict['blank'].append(1)\n",
    "                    else:\n",
    "                        chip_dict['blank'].append(0)\n",
    "\n",
    "            return chip_dict\n",
    "\n",
    "        def __read_raster(self,raster):\n",
    "            # check whether raster is a path or array\n",
    "            if isinstance(raster,(pathlib.PurePath,str)):\n",
    "                    with rio.open(raster) as r:\n",
    "                        # convert raster into np array\n",
    "                        np_array = r.read()\n",
    "                    return np_array\n",
    "\n",
    "            elif isinstance(raster,np.ndarray):\n",
    "                return raster\n",
    "            else:\n",
    "                raise ValueError(f\"Expected Path or Numpy array received: {type(raster)}\")  \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb131ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "# https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\n",
    "ENCODER = 'vgg16'#'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['Buildings']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2bd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "        A.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return A.Compose(_transform)\n",
    "\n",
    "train_set   = Dataloader_trdp_pre_trained(root_dir=train_dir,csv_file=csv_file,\n",
    "                              chip_dimension=chip_dimension,\n",
    "                              preprocessing=get_preprocessing(preprocessing_fn),\n",
    "                              transform=transform)\n",
    "\n",
    "testing_set = Dataloader_trdp_pre_trained(root_dir=test_dir,\n",
    "                              csv_file=csv_file_test,\n",
    "                              chip_dimension=chip_dimension,\n",
    "                              preprocessing=get_preprocessing(preprocessing_fn),transform=transform)\n",
    "\n",
    "#train_set, val_set = torch.utils.data.random_split(train_set, [round(0.7*len(train_set)),round(0.3*len(train_set))])\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle = False, num_workers=12)\n",
    "#val_loader   = DataLoader(dataset = val_set, batch_size=batch_size, shuffle = False)\n",
    "test_loader  = DataLoader(dataset = testing_set, batch_size=batch_size, shuffle = False, num_workers=4)\n",
    "\n",
    "print(f\"Train : {len(train_loader)} - Test: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef19c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "image, mask = train_set[4] # get some sample\n",
    "\n",
    "#image = sample[\"raster_diff\"]\n",
    "#mask = sample[\"mask_diff\"]\n",
    "print(f\"{image.shape} - {mask.shape}\")\n",
    "visualize(\n",
    "    image=image[1,:,:].squeeze(), \n",
    "    Building_mask=mask.squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(train_set): {len(train_set)}\")\n",
    "#print(f\"len(testing_set): {len(testing_set)}\")\n",
    "\n",
    "for i in range(len(train_set)):\n",
    "    image, mask = train_set[i]\n",
    "    # verify dimensionality\n",
    "    print(image.shape, mask.shape)\n",
    "    if i==2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc928582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "\n",
    "for i in range(0, 3):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(test_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "best_model = torch.load('./best_model.pth')\n",
    "\n",
    "# evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.random.choice(len(testing_set))\n",
    "    \n",
    "    image_vis = testing_set[n][0].astype('uint8')\n",
    "    image, gt_mask = testing_set[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis[1,:,:], \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=pr_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb1b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a8b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e631de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd058f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fe92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011a9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7da172",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc62bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3875a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdd83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a92d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562872f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d9dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
