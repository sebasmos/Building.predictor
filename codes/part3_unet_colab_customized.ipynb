{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HtE6X7st0DB3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtE6X7st0DB3",
    "outputId": "ba48c423-7f86-4cfd-9c8d-eed35bb6eb5b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OwrQlXN80Fbp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwrQlXN80Fbp",
    "outputId": "c501cad9-f54b-44e4-d776-2d5f1677ec63"
   },
   "outputs": [],
   "source": [
    "!pip install -U albumentations --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "E_l3XWmW0LiZ",
   "metadata": {
    "id": "E_l3XWmW0LiZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZDZPv4CU0J-j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDZPv4CU0J-j",
    "outputId": "1a2b913d-ba60-42cf-8dff-9164f3544bee"
   },
   "outputs": [],
   "source": [
    "% cd /content/drive/MyDrive/Colab Notebooks/SPACENET7/Multi-temporal.building.tracker/codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff058fd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff058fd0",
    "outputId": "3ca83d06-6af5-4658-9288-b468f84c72ab"
   },
   "outputs": [],
   "source": [
    "!pip install rasterio\n",
    "!pip install patchify\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Y2Lr8npCB-VT",
   "metadata": {
    "id": "Y2Lr8npCB-VT"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "required = {'opencv-python'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing], stdout=None)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "from patchify import patchify, unpatchify\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# Albumentations\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from model import UNET\n",
    "\n",
    "from data_loader import Dataloader_trdp\n",
    "from utils import (\n",
    "    load_checkpoint,\n",
    "    load_checkpoint_pspnetlite,\n",
    "    save_checkpoint,\n",
    "    #get_loaders,\n",
    "    check_accuracy,\n",
    "    save_predictions_as_imgs,\n",
    "    predict,\n",
    "    store_predictions,\n",
    "    store_predictions_with_patching,\n",
    "    store_predictions_unet_improved\n",
    ")\n",
    "\n",
    "# Clean graphics memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e986c2e9",
   "metadata": {
    "id": "e986c2e9"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 1\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "chip_dimension = 256\n",
    "LOAD_MODEL = False\n",
    "TRAIN = True\n",
    "TEST = False\n",
    "SAVE = False\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "PREDICT = True\n",
    "\n",
    "filename = \"unet_23_dic_draft_dataset_customized_unet.pth\" # my_checkpoint.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "_BWBdGPhChzp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BWBdGPhChzp",
    "outputId": "8c39db0e-d73e-43ab-e3e9-50babf5e20b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "% cd /content/drive/MyDrive/Colab Notebooks/SPACENET7/Multi-temporal.building.tracker/codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d7b7d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25d7b7d5",
    "outputId": "7bd8fa79-bbe5-4cf6-ebe6-a8bd534aeb58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 1504 - Test: 624\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "train_dir = Path('/content/drive/MyDrive/Colab Notebooks/SPACENET7/DATASET/dataset_pruebas/train')\n",
    "test_dir = Path('/content/drive/MyDrive/Colab Notebooks/SPACENET7/DATASET/dataset_pruebas/validation')\n",
    "\n",
    "\n",
    "root_dir  = train_dir\n",
    "csv_file = Path('/content/drive/MyDrive/Colab Notebooks/SPACENET7/DATASET/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "csv_file_test = Path('/content/drive/MyDrive/Colab Notebooks/SPACENET7/DATASET/output_csvs_dataset_prueba/df_test_untidy.csv')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#TRAINING\n",
    "#train_dir = Path('../../DATASET/archive/train_final')\n",
    "#csv_file = Path('./output_csvs_all/df_train_untidy.csv')\n",
    "train_dir  = Path('../../DATASET/dataset_pruebas/train')\n",
    "csv_file = Path('./registros/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "\n",
    "#TESTING\n",
    "test_dir  = Path('../../DATASET/dataset_pruebas/validation')\n",
    "csv_file_test = Path('./registros/output_csvs_dataset_prueba/df_test_untidy.csv')\n",
    "#test_dir  = Path('../../DATASET/archive/val_final')\n",
    "#csv_file = Path('./registros/output_csvs_dataset_prueba/df_train_untidy.csv')\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df_test = pd.read_csv(csv_file_test)\n",
    "# Reconstructiontest_loader\n",
    "img_size = 1024\n",
    "# Chip size given batch_size\n",
    "chip_dim = ((img_size -1)//batch_size + 1)*2\n",
    "# number of Columns per chip\n",
    "columns = img_size / chip_dim\n",
    "# Needed patches to reconstruct original image\n",
    "patches_total = int(columns**2)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "A.Normalize(mean=mean,std=std)\n",
    "#A.Rotate(limit=(-360, 360), interpolation=4, border_mode=4,p=1),\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        #A.Resize(height=256, width=256),\n",
    "        \n",
    "        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n",
    "        #A.RandomRotate90(p=1.0),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set   = Dataloader_trdp(root_dir=train_dir,csv_file=csv_file,chip_dimension=chip_dimension,transform=transform)\n",
    "\n",
    "testing_set = Dataloader_trdp(root_dir=test_dir,csv_file=csv_file_test,chip_dimension=chip_dimension,transform=transform)\n",
    "\n",
    "#train_set, val_set = torch.utils.data.random_split(train_set, [round(0.7*len(train_set)),round(0.3*len(train_set))])\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size=batch_size, shuffle = False)\n",
    "#val_loader   = DataLoader(dataset = val_set, batch_size=batch_size, shuffle = False)\n",
    "test_loader  = DataLoader(dataset = testing_set, batch_size=batch_size, shuffle = False)\n",
    "\n",
    "print(f\"Train : {len(train_loader)} - Test: {len(test_loader)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee36b193",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "ee36b193",
    "outputId": "78130b99-265c-44cb-f5b0-5534416d91d8"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1025418016.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_369715/1025418016.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    ]\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "train_transform_train = A.Compose(\n",
    "        [\n",
    "            #A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "transform_test = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n",
    "        ToTensorV2()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "P909KfUtD-Yx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P909KfUtD-Yx",
    "outputId": "d90dc157-c733-4728-ede5-23598e7e7a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "\n",
    "def double_block(in_c, out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=3),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Conv2d(out_c, out_c, kernel_size=3),\n",
    "        nn.ReLU(inplace = True),\n",
    "        \n",
    "        )\n",
    "    return conv\n",
    "    \n",
    "\n",
    "class unet_bas(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(unet_bas, self).__init__()\n",
    "        \n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        self.contractor_1 = double_block(3, 64)\n",
    "        self.contractor_2 = double_block(64, 128)\n",
    "        self.contractor_3 = double_block(128, 256)\n",
    "        self.contractor_4 = double_block(256, 512)\n",
    "        self.contractor_5 = double_block(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(in_channels = 1024, out_channels=512, kernel_size = 2, stride=2)\n",
    "        self.up_conv_1 = double_block(1024, 512)\n",
    "        \n",
    "                \n",
    "        self.up_trans_2 = nn.ConvTranspose2d(in_channels = 512, out_channels=256, kernel_size = 2, stride=2)\n",
    "        self.up_conv_2 = double_block(512, 256)\n",
    "        \n",
    "                \n",
    "        self.up_trans_3 = nn.ConvTranspose2d(in_channels = 256, out_channels=128, kernel_size = 2, stride=2)\n",
    "        self.up_conv_3 = double_block(256, 128)\n",
    "        \n",
    "                \n",
    "        self.up_trans_4 = nn.ConvTranspose2d(in_channels = 128, out_channels=64, kernel_size = 2, stride=2)\n",
    "        self.up_conv_4 = double_block(128, 64)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels = 64, out_channels = 1, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        # Encoder\n",
    "        # image (1, 3, 256, 256)\n",
    "        x1 = self.contractor_1(image) # (1, 64, 252, 252)\n",
    "        x2 = self.max_pool_2x2(x1) #(1, 128, 126, 126)\n",
    "        x3 = self.contractor_2(x2)# (1, 128, 122, 122)\n",
    "        x4 = self.max_pool_2x2(x3)# (1, 128, 61, 61)\n",
    "        x5 = self.contractor_3(x4)# (1, 256, 57, 57)\n",
    "        x6 = self.max_pool_2x2(x5)# (1, 256, 28, 28) -> 28.5\n",
    "        x7 = self.contractor_4(x6)# \n",
    "        x8 = self.max_pool_2x2(x7)\n",
    "        x9 = self.contractor_5(x8)\n",
    "        \n",
    "        #Decoder\n",
    "\n",
    "        x = self.up_trans_1(x9)\n",
    "        y = crop_img(x7, x)\n",
    "        if y.shape != x.shape:\n",
    "        \ty = TF.resize(y, x.shape[2:])\n",
    "        x = self.up_conv_1(torch.cat([x,y],1))\n",
    "        \n",
    "        x = self.up_trans_2(x)\n",
    "        y = crop_img(x5, x)#torch.Size([1, 256, 25, 25])\n",
    "        if y.shape != x.shape:\n",
    "        \ty = TF.resize(y, x.shape[2:])\n",
    "        x = self.up_conv_2(torch.cat([x,y],1))\n",
    "        \n",
    "        x = self.up_trans_3(x)\n",
    "        y = crop_img(x3, x)\n",
    "        if y.shape != x.shape:\n",
    "        \ty = TF.resize(y, x.shape[2:])\n",
    "        x = self.up_conv_3(torch.cat([x, y],1))\n",
    "        \n",
    "        \n",
    "        x = self.up_trans_4(x)\n",
    "        y = crop_img(x1, x)\n",
    "        if y.shape != x.shape:\n",
    "        \ty = TF.resize(y, x.shape[2:])\n",
    "        x = self.up_conv_4(torch.cat([x,y],1))\n",
    "        \n",
    "        # machetazo\n",
    "        \n",
    "        x = TF.resize(x, (256,256))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "def crop_img(tensor, target_tensor):\n",
    "\t'''\n",
    "\tFrom original paper: take image of size 64 from 4th stage and crop it to fit with\n",
    "\tupconvoluted image of size 56. \n",
    "\te.g.\n",
    "\ttarget_size = 56\n",
    "\ttensor_size = 64\n",
    "\tdelta = 8 // 2 # 4 \n",
    "\t\n",
    "\t#output\n",
    "\ttensor[:,:,delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "\ttensor[:,:,4:64-4, 4:64-4]\n",
    "\ttensor[:,:,4:60, 4:60]\n",
    "\t\n",
    "\t# as a result, cropping is centered\n",
    "\t\n",
    "\t'''\n",
    "\ttarget_size = target_tensor.size()[2]\n",
    "\t\n",
    "\ttensor_size = tensor.size()[2]\n",
    "\t\n",
    "\tdelta = tensor_size - target_size\n",
    "\n",
    "\tdelta = delta//2\n",
    "\t\n",
    "\tx = tensor[:,:,delta:tensor_size-delta, delta:tensor_size-delta]\n",
    "\treturn  x\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "test_mask = torch.randn((1, 3, 256, 256)) # RGB image\n",
    "    ## MODEL 1\n",
    "    #model = UNET(in_channels=1, out_channels=1)v\n",
    "    #model = UNET_manual(3, 1)\n",
    "model = unet_bas()\n",
    "preds = model(test_mask)\n",
    "print(preds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94276746",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94276746",
    "outputId": "137fadae-4104-41a9-879b-7dd12e7fa08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.10.0\n",
      "Total params: 31M\n",
      "Unet improved --> training, epoch 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import numpy as np\n",
    "    \n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "print('PyTorch version:', torch.__version__)\n",
    "\n",
    "\n",
    "# https://github.com/SpaceNetChallenge/SpaceNet_SAR_Buildings_Solutions/blob/6a9c3962d987d985384d0d41a187f5fbfadac82c/2-MaksimovKA/train/losses.py#L14 \n",
    "class FocalDiceLoss(torch.nn.Module):\n",
    "    def __init__(self, coef_focal=1.0, coef_dice=1.0, weights=(1.0, 0.1, 0.5)):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.weights = weights\n",
    "\n",
    "        self.coef_focal = coef_focal\n",
    "        self.coef_dice = coef_dice\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = 0.0\n",
    "\n",
    "        for i in range(outputs.shape[1]):\n",
    "            dice = self.weights[i]*self.dice_loss(outputs[:, i, ...], targets[:, i, ...])\n",
    "            focal = self.weights[i]*self.focal_loss(outputs[:, i, ...], targets[:, i, ...])\n",
    "            loss += self.coef_dice * dice + self.coef_focal * focal\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    \n",
    "class FocalLoss(_Loss):\n",
    "\n",
    "    def __init__(self, ignore_index=255, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        :param y_pred: NxCxHxW\n",
    "        :param y_true: NxCxHxW\n",
    "        :return: scalar\n",
    "        \"\"\"\n",
    "        y_pred = y_pred.sigmoid()\n",
    "        gamma = self.gamma\n",
    "        ignore_index = self.ignore_index\n",
    "\n",
    "        outputs = y_pred.contiguous()\n",
    "        targets = y_true.contiguous()\n",
    "        eps = 1e-8\n",
    "        non_ignored = targets.view(-1) != ignore_index\n",
    "        targets = targets.view(-1)[non_ignored].float()\n",
    "        outputs = outputs.contiguous().view(-1)[non_ignored]\n",
    "        outputs = torch.clamp(outputs, eps, 1. - eps)\n",
    "        targets = torch.clamp(targets, eps, 1. - eps)\n",
    "        pt = (1 - targets) * (1 - outputs) + targets * outputs\n",
    "        return (-(1. - pt) ** gamma * torch.log(pt)).mean()\n",
    "\n",
    "\n",
    "class DiceLoss(_Loss):\n",
    "\n",
    "    def __init__(self, per_image=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        :param y_pred: NxCxHxW\n",
    "        :param y_true: NxCxHxW\n",
    "        :return: scalar\n",
    "        \"\"\"\n",
    "        per_image = self.per_image\n",
    "        y_pred = y_pred.sigmoid()\n",
    "        \n",
    "        batch_size = y_pred.size()[0]\n",
    "        eps = 1e-5\n",
    "        if not per_image:\n",
    "            batch_size = 1\n",
    "        \n",
    "        dice_target = y_true.contiguous().view(batch_size, -1).float()\n",
    "        dice_output = y_pred.contiguous().view(batch_size, -1)\n",
    "        intersection = torch.sum(dice_output * dice_target, dim=1)\n",
    "        union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n",
    "        loss = (1 - (2 * intersection + eps) / union).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class configuration:\n",
    "    def __init__(self):\n",
    "        self.experiment_name = \"trdp1.001\"\n",
    "        self.pre_load = \"True\" ## Load dataset in memory\n",
    "        self.pre_trained = \"True\"\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_label = 255\n",
    "        self.lr = learning_rate  # 0.001 if pretrained. 0.1 if scratch\n",
    "        self.M = [] ##If training from scratch, reduce learning rate at some point\n",
    "        self.batch_size = batch_size  # Training batch size\n",
    "        self.test_batch_size = 4  # Test batch size\n",
    "        self.epoch = num_epochs ## Number of epochs\n",
    "        self.train_root = \"./VOC\"\n",
    "        self.download = False\n",
    "        self.seed = 271828\n",
    "\n",
    "\n",
    "## Create arguments object\n",
    "args = configuration()\n",
    "\n",
    "# Make sure to enable GPU acceleration!\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Set random seed for reproducability\n",
    "torch.backends.cudnn.deterministic = True  # fix the GPU to deterministic mode\n",
    "torch.manual_seed(args.seed)  # CPU seed\n",
    "torch.cuda.manual_seed_all(args.seed)  # GPU seed\n",
    "random.seed(args.seed)  # python seed for image transformation\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "def _fast_hist(label_pred, label_true, num_classes):\n",
    "    mask = (label_true >= 0) & (label_true < num_classes)\n",
    "    hist = np.bincount(\n",
    "        num_classes * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)\n",
    "    return hist\n",
    "\n",
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"Fast enough iou calculation function\"\"\"\n",
    "    SMOOTH = 1e-6\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    #outputs = outputs.detach()\n",
    "    #labels = labels.detach()\n",
    "    \n",
    "    intersection = (outputs & labels).float().sum((1, 2))\n",
    "    union = (outputs | labels).float().sum((1, 2))\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded.mean() # to get a batch averageimport pdb\n",
    "\n",
    "def iou_fun(im1, im2, empty_score=1.0):\n",
    "    im1 = np.asarray(im1).astype(np.bool)\n",
    "    im2 = np.asarray(im2).astype(np.bool)\n",
    "\n",
    "    if im1.shape != im2.shape:\n",
    "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
    "\n",
    "    union = np.logical_or(im1, im2)\n",
    "    im_sum = union.sum()\n",
    "    if im_sum == 0:\n",
    "        return empty_score\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    intersection = np.logical_and(im1, im2)\n",
    "\n",
    "    return intersection.sum() / im_sum\n",
    "def train_SemanticSeg(args, model, device, train_loader, optimizer, epoch, criterion):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    counter = 1\n",
    "\n",
    "    gts_all, predictions_all = [], []\n",
    "\n",
    "    for batch_idx, (data) in enumerate(train_loader):\n",
    "        \n",
    "        images = data[\"raster_diff\"].float()\n",
    "        \n",
    "        mask = data[\"mask_diff\"].float().squeeze()\n",
    "        \n",
    "        images, mask = images.to(device), mask.to(device)\n",
    "\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            \n",
    "        \n",
    "        #Forward pass\n",
    "        #print(images.size())\n",
    "        outputs = model(images).squeeze()\n",
    "        #print(outputs.size())\n",
    "                \n",
    "        #Aggregated per-pixel loss\n",
    "        #loss = criterion(outputs, mask)\n",
    "        \n",
    "        loss = criterion(outputs.unsqueeze(0).unsqueeze(0), mask.unsqueeze(0).unsqueeze(0))\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        iou = iou_fun(mask.data.cpu().numpy(), outputs.data.cpu().numpy())#iou_pytorch(mask.unsqueeze(0).int(), outputs.int())\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % 10000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Learning rate: {:.6f}'.format(\n",
    "                epoch, int(counter * len(images)), len(train_loader.dataset),\n",
    "                100. * counter / len(train_loader), loss.item(),\n",
    "                optimizer.param_groups[0]['lr']))\n",
    "            print(f\"Iou: {iou}\")\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return sum(train_loss) / len(train_loss)#, mean_iu\n",
    "        \n",
    "def testing(args, model, device, test_loader, criterion):\n",
    "\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    loss_per_batch = []\n",
    "    test_loss = 0\n",
    "    \n",
    "    gts_all, predictions_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data) in enumerate(test_loader):\n",
    "            \n",
    "            print(f\" {batch_idx}/{len(test_loader)} \")\n",
    "            if batch_idx <= 992:\n",
    "            \n",
    "                images = data[\"raster_diff\"].float()\n",
    "                mask = data[\"mask_diff\"].float()#type(torch.LongTensor)\n",
    "                images, mask = images.to(device), mask.to(device).squeeze()\n",
    "                #Forward pass\n",
    "                    \n",
    "                if torch.cuda.is_available():\n",
    "                    model.cuda()\n",
    "                outputs = model(images).squeeze()\n",
    "                #outputs = outputs.clone().detach().cpu().numpy()\n",
    "        #        outputs = outputs.cpu().numpy()\n",
    "                outputs = ((outputs - outputs.min())/(outputs.max()-outputs.min()))\n",
    "                #outputs = np.round(outputs)\n",
    "\n",
    "                outputs[outputs>THRESHOLD] = 1 \n",
    "                outputs[outputs<=THRESHOLD] = 0\n",
    "                #outputs = torch.round(outputs)\n",
    "\n",
    "                #Aggregated per-pixel loss\n",
    "                #loss = criterion(outputs, mask)\n",
    "                \n",
    "                loss = criterion(outputs.unsqueeze(0).unsqueeze(0), mask.unsqueeze(0).unsqueeze(0))\n",
    "                loss_per_batch.append(loss.item())\n",
    "\n",
    "                # Convert to numpy\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                outputs = outputs.astype(\"int64\")\n",
    "                #predictions = np.round(predictions)\n",
    "\n",
    "                mask = mask.data.cpu().numpy()\n",
    "                mask = mask.astype(\"int64\")\n",
    "\n",
    "                '''\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.imshow(outputs)\n",
    "                np.unique(outputs)\n",
    "\n",
    "                import seaborn as sns\n",
    "                sns.histplot(data = outputs)\n",
    "\n",
    "                preds = outputs.data.max(1)[1].squeeze(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                mask = mask.data.cpu().numpy()\n",
    "                mask = mask.astype(\"int64\")\n",
    "                plt.imshow(mask)\n",
    "\n",
    "                assert np.unique(mask) == np.unique(outputs*255)\n",
    "                '''\n",
    "                gts_all.append(mask)\n",
    "                predictions_all.append(255*outputs)\n",
    "\n",
    "                if SAVE:\n",
    "\n",
    "                    images = \"outputs\"\n",
    "                    labels = images+\"/predictions\"\n",
    "                    true = images+\"/labels\"\n",
    "\n",
    "                    if not os.path.exists(images):\n",
    "                        os.makedirs(images)\n",
    "                    if not os.path.exists(labels):\n",
    "                            os.makedirs(labels)\n",
    "                    if not os.path.exists(true):\n",
    "                            os.makedirs(true)\n",
    "\n",
    "                    matplotlib.image.imsave(f\"{labels}/Pred_{batch_idx}.png\", outputs , cmap='gray')\n",
    "\n",
    "                    matplotlib.image.imsave(f\"{true}/True_{batch_idx}.png\", mask, cmap='gray')\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "    loss_per_epoch = [np.average(loss_per_batch)]\n",
    "    \n",
    "\n",
    "    ##Compute Mean Intersection over Union (mIoU)\n",
    "    ##mIoU: Mean (of all classes) of intersection over union between prediction\n",
    "    ##and ground-truth\n",
    "\n",
    "    # Improved: https://towardsdatascience.com/intersection-over-union-iou-calculation-for-evaluating-an-image-segmentation-model-8b22e2e84686 \n",
    "    iou_scores = []\n",
    "    for lp, lt in zip(predictions_all, gts_all):\n",
    "        # Convert lp to cpu\n",
    "        #lp = lp.detach().cpu().numpy()\n",
    "        intersection = np.logical_and(lp.flatten(), lt.flatten())  \n",
    "        union = np.logical_or(lp.flatten(), lt.flatten())  \n",
    "        iou_score = np.sum(intersection) / np.sum(union)\n",
    "        iou_scores.append(iou_score)\n",
    "    \n",
    "    mean_iu = np.average(iou_scores)\n",
    "    \n",
    "    print('\\nTest set ({:.0f}): Average loss: {:.4f}, mIoU: {:.4f}\\n'.format(\n",
    "        len(test_loader.dataset), loss_per_epoch[-1], mean_iu)) \n",
    "    \n",
    "    ###########################################################################\n",
    "    # Store predictions\n",
    "\n",
    "    return loss_per_epoch, mean_iu, predictions_all, gts_all\n",
    "    \n",
    "\n",
    "model = unet_bas().to(device)\n",
    "\n",
    "criterion = FocalDiceLoss()# nn.BCEWithLogitsLoss()\n",
    "print('Total params: %2.fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "    \n",
    "milestones = args.M\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n",
    "\n",
    "import os\n",
    "import time\n",
    "loss_train_epoch = []\n",
    "loss_test_epoch = []\n",
    "acc_train_per_epoch = []\n",
    "acc_test_per_epoch = []\n",
    "new_labels = []\n",
    "\n",
    "cont = 0\n",
    "\n",
    "res_path = \"./metrics_\" + args.experiment_name\n",
    "\n",
    "\n",
    "model_path = \"./models\" # + filename\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    print(\"models folder created\")\n",
    "\n",
    "if LOAD_MODEL:\n",
    "        load_checkpoint_pspnetlite(torch.load(\"models/\" + filename), model)\n",
    "    \n",
    "\n",
    "if not os.path.isdir(res_path):\n",
    "    os.makedirs(res_path)\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "        st = time.time()\n",
    "        scheduler.step()    \n",
    "        # train for one epoch\n",
    "        if TRAIN:\n",
    "\n",
    "            print(\"Unet improved --> training, epoch \" + str(epoch))\n",
    "    \n",
    "            loss_per_epoch = train_SemanticSeg(args, model, device, train_loader, optimizer, epoch, criterion)\n",
    "    \n",
    "            loss_train_epoch += [loss_per_epoch]\n",
    "            \n",
    "            if epoch == args.epoch:\n",
    "                torch.save(model.state_dict(), \"models/\" + filename)\n",
    "    \n",
    "            np.save(res_path + '/' + 'LOSS_epoch_train.npy', np.asarray(loss_train_epoch))\n",
    "        \n",
    "        # TESTING\n",
    "        if TEST:\n",
    "            print(\"Unet improved ==> testing, epoch \" + str(epoch))\n",
    "            # test\n",
    "            loss_per_epoch_test, acc_val_per_epoch_i, a,b = testing(args, model, device, test_loader, criterion)\n",
    "    \n",
    "            loss_test_epoch += loss_per_epoch_test\n",
    "            acc_test_per_epoch += [acc_val_per_epoch_i]\n",
    "    \n",
    "    \n",
    "            if epoch == 1:\n",
    "                best_acc_val = acc_val_per_epoch_i\n",
    "    \n",
    "            else:\n",
    "                if acc_val_per_epoch_i > best_acc_val:\n",
    "                    best_acc_val = acc_val_per_epoch_i\n",
    "    \n",
    "    \n",
    "            np.save(res_path + '/' + 'LOSS_epoch_val.npy', np.asarray(loss_test_epoch))\n",
    "    \n",
    "            # save accuracies:\n",
    "            np.save(res_path + '/' + 'accuracy_per_epoch_val.npy', np.asarray(acc_test_per_epoch))\n",
    "            \n",
    "            cont += 1\n",
    "\n",
    "    \n",
    "# PREDICT\n",
    "\n",
    "if PREDICT: \n",
    "    \n",
    "    y_pred, true_pred = [], []\n",
    "    \n",
    "    loss_per_epoch_test, acc_val_per_epoch_i, y_pred, true_pred = testing(args, model, device, test_loader, criterion)\n",
    "    \n",
    "    y_pred = np.array(y_pred)\n",
    "    true_pred = np.array(true_pred)\n",
    "    \n",
    "    print(f\"y_pred sizes: {y_pred.shape} - true_pred sizes: {true_pred.shape}\")\n",
    "    print(f\"y_pred pixel values: {np.unique(y_pred)}\")\n",
    "\n",
    "    store_predictions_with_patching(y_pred, true_pred, 16)\n",
    "\n",
    "\n",
    "##Accuracy\n",
    "acc_test = np.load(res_path + '/' + 'accuracy_per_epoch_val.npy')\n",
    "\n",
    "#Loss per epoch\n",
    "loss_train = np.load(res_path + '/' + 'LOSS_epoch_train.npy')\n",
    "loss_test = np.load(res_path + '/' + 'LOSS_epoch_val.npy')\n",
    "\n",
    "numEpochs = len(acc_test)\n",
    "epochs = range(numEpochs)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(epochs, acc_test, label='Test, max acc: ' + str(np.max(acc_test)))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(epochs, loss_test, label='Test, min loss: ' + str(np.min(loss_test)))\n",
    "plt.plot(epochs, loss_train, label='Train, min loss: ' + str(np.min(loss_train)))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KtaWlxN2F8Vw",
   "metadata": {
    "id": "KtaWlxN2F8Vw"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "part3_unet_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
